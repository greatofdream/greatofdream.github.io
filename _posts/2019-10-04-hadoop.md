---
layout: post
title:
date: 2019-10-04
---
## hadoop:pill:

位于etc/hadoop-env.sh中的**JAVA_HOME**似乎应该需要[配置绝对路径](https://stackoverflow.com/questions/20628093/java-home-is-not-set-in-hadoop)

使用hadoop3.2.1时，openjdk9似乎有问题，使用yarn时会报错声称
```
2019-10-04 16:21:59,566 INFO mapreduce.Job: Job job_1570176818335_0003 failed with state FAILED due to: 
Application application_1570176818335_0003 failed 2 times due to Error launching appattempt_1570176818335_0003_000002. 
Got exception: org.apache.hadoop.security.AccessControlException: Unable to find SASL server implementation for DIGEST-MD5
```
换成openjdk8
```
[2019-10-04 17:17:02.073]Container exited with a non-zero exit code 127. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
/bin/bash: /bin/java: No such file or directory
```

换成oracle jdk8
报错和openjdk8一致
只好强行在/bin/中写入文件连接
```
ls -s /usr/bin/java /bin/java
```
在另一台机器配置了host文件，将对应的域名指向127.0.0.1，发现竟然没有这个错误。

配置分布式后发现启动没有Namenode,resourcemanager，果断将/etc/hosts域名指向127.0.0.1，解决了这个问题，重新启动就正常了


重启会报错
```
 org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-ubuntu/dfs/name is in an inconsistent state
```
有问答指出[tmp会被清理](https://stackoverflow.com/questions/17376982/org-apache-hadoop-hdfs-server-common-inconsistentfsstateexception-directory-tm)
因此调整core-site.xml中对于**tmp.dir**设置

发现host配置错误会卡在resourcesmanager上，本机名字对应的ip一定要是**127.0.0.1**放在第一位
否则报错
```
 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
 ```
 
